{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYCx/D9xtkh6KDL7z4ufP1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Modelos"],"metadata":{"id":"wlCDZPzSqqoJ"}},{"cell_type":"markdown","source":["Implementaremos distintos modelos y compararemos su desempeño para trabajar el problema planteado sobre el Banknote authentication dataset. Este dataset contiene 1372 muestras de billetes reales, y cuenta con los siguientes atributos:\n","\n","*   Variance of Wavelet Transformed image (continuous): Hace referencia a la variación de las intensidades de los pixeles de la imagen de la transformada wavelet.\n","*   Skewness of Wavelet Transformed image (continuous): Mide la antisimetría de la distribución de los valores obtenidos al aplicar la transformada wavelet en la imagen (coeficientes de wavelet). Si el skewness es igual a cero, entonces es una distribución simétrica.\n","*   Curtosis of Wavelet Transformed image (continuous): Busca determinar la forma de la distribución de coeficientes wavelet.\n","*   Entropy of image (continuous): Mide la aleatoriedad de la frecuencia en el contenido de la imagen.\n","\n","Las etiquetas del dataset están dadas por billetes auténticos representados por 0, y billetes falsos representados por 1."],"metadata":{"id":"80LJfRGerCN1"}},{"cell_type":"markdown","source":["**(1) Modelo de Regresión lineal**\n","\n","Para este dataset el objetivo es predecir si un billete es falso o verdadero, por lo tanto si queremos realizar un modelo de regresión lineal podemos abordarlo modificando la variable objetivo. En lugar de predecir si el billete es verdadero o falso podemos transformar la variable objetivo en una variable continua que represente un puntage de seguridad o una probabilidad de que el billete sea genuino. Sin embargo, al transformar el problema podemos afectar la interpretabilidad y la utilidad del modelo. Además de esto, la regresión lineal asume una relación lineal entre las features y la función de target lo cual puede o no ser cierto para este dataset. \n","\n","A continuación se muestra la implementación del modelo."],"metadata":{"id":"N7pBmwm3ruya"}},{"cell_type":"code","source":["# Emplearemos la librería Pandas ya que esta se especializa en el manejo \n","# y análisis de estructuras de datos. Esta librería nos permitirá cargar el dataset\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","# Ahora cargaremos el dataset en un Dataframe que nos permitirá destacar las \n","#relaciones entre las variables de los datos.\n","data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\",\n","                   header=None)\n","# Luego asignaremos los nombres de cada columna según el feature correspondiente,\n","# donde la última columna corresponderá a las etiquetas.\n","data.columns = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"]"],"metadata":{"id":"b5V6-jlpnRrE","executionInfo":{"status":"ok","timestamp":1685502012601,"user_tz":300,"elapsed":480,"user":{"displayName":"Laura Catalina Garcia Arevalo","userId":"14956040326510211844"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Asignamos features a X\n","X = data[[\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]]\n","\n","# Asignamos la variable objetivo a  y\n","y = data[\"Class\"]"],"metadata":{"id":"iNoz08gQnyC4","executionInfo":{"status":"ok","timestamp":1685502035732,"user_tz":300,"elapsed":189,"user":{"displayName":"Laura Catalina Garcia Arevalo","userId":"14956040326510211844"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#Inicialmente realizaremos el modelo empleando las librerías de Sklearn \n","import numpy as np\n","#Ejecutaremos el modelo de regresión usando 'Ridge', el cual es un modelo de \n","# regresión que permite hacer regularización\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import cross_val_score\n","#La siguiente librería la empleamos para estandarizar las features\n","from sklearn.preprocessing import StandardScaler\n","\n","# Estandarizamos las características\n","#Modificamos la variable objetivo para realizar la regresión\n","y_regression = y / (y.max() - y.min())\n","# Estandarizamos las características\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Definimos los hiperparámetros\n","alpha = 0.1  # Parámetro de regularización\n","\n","# Creamos una instancia del modelo Ridge Regression con regularización \n","model = Ridge(alpha=alpha)\n","\n","# Realizamos validación cruzada para garantizar generalización\n","cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n","\n","# Convertimos los MSE (mean squared error) a valores positivos\n","mse_scores = -cv_scores\n","\n","# Calculamos el error cuadrado promedio sobre todos los folds\n","average_mse = np.mean(mse_scores)\n","print(\"Average Mean Squared Error:\", average_mse)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kf55TjnVqZj7","executionInfo":{"status":"ok","timestamp":1685501222178,"user_tz":300,"elapsed":217,"user":{"displayName":"Laura Catalina Garcia Arevalo","userId":"14956040326510211844"}},"outputId":"8a60b218-2843-4ede-d84d-33dbd107b6de"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Mean Squared Error: 0.03857964683034848\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","# Definimos los hiperparámetros \n","learning_rate = 0.01\n","num_iterations = 1000\n","regularization_param = 0.01\n","num_folds = 5 #Aumentar el número de folds brinda una evaluzación más detallada del \n","#modelo, pero puede requerir más cómputo\n","\n","# Inicializamos un arreglo para guardar los errores de mínimos cuadrados\n","mse_scores = []\n","\n","# Ejecutamos cross-validation\n","#El dataset es dividido en el número de folds, donde cada fold es usado una vez\n","# como conjunto de entrenamiento \n","fold_size = len(X) // num_folds\n","for i in range(num_folds):\n","    # Dividimos los datos en datos de entrenamiento y de prueba para el fold correspondiente\n","    X_train = np.concatenate((X[:i * fold_size], X[(i + 1) * fold_size:]), axis=0)\n","    y_train = np.concatenate((y[:i * fold_size], y[(i + 1) * fold_size:]), axis=0)\n","    X_test = X[i * fold_size:(i + 1) * fold_size]\n","    y_test = y[i * fold_size:(i + 1) * fold_size]\n","\n","    # Inicializamos los pesos \n","    num_features = X_train.shape[1]\n","    weights = np.zeros(num_features)\n","\n","    # Definimos la función de costo (error de mínimos cuadrados)\n","    def mean_squared_error(X, y, weights, regularization_param):\n","        predictions = np.dot(X, weights)\n","        errors = predictions - y\n","        mse = np.mean(errors ** 2) + regularization_param * np.sum(weights[1:] ** 2)\n","        return mse\n","\n","    # Realizamos gradiente en descenso para optimizar los pesos\n","    for iteration in range(num_iterations):\n","        predictions = np.dot(X_train, weights)\n","        errors = predictions - y_train\n","        gradient = np.dot(X_train.T, errors) / len(X_train) + 2 * regularization_param * weights\n","        weights -= learning_rate * gradient\n","\n","    # Calculamos el error de mínimos cuadrados para el fold actual\n","    mse = mean_squared_error(X_test, y_test, weights, regularization_param)\n","    mse_scores.append(mse)\n","\n","# Calculamos el promedio del error de mínimos cuadrados sobre todos los folds\n","average_mse = np.mean(mse_scores)\n","print(\"Average Mean Squared Error:\", average_mse)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YuO4y1drNQr","executionInfo":{"status":"ok","timestamp":1685499249545,"user_tz":300,"elapsed":181,"user":{"displayName":"Laura Catalina Garcia Arevalo","userId":"14956040326510211844"}},"outputId":"4ec61d68-9a63-4335-c5f7-f2e58160b6a8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Mean Squared Error: 0.3348471965402167\n"]}]},{"cell_type":"markdown","source":["**(ii) Regresión logística**\n","\n","La regresión logística es un algoritmo popular para realizar clasificación binaria, por lo tanto, a diferencia del modelo anterior, puede ser pertinente implementarlo para el dataset que estamos trabajando. Para ajustar los hiperparámetros en estos modelos se suelen usar distintos métodos, tales como Regularization Strength, solver algorithm, class wighting, entre otros. En el caso particular del modelo implementado se tiene que que Aumentar el número de folds brinda una evaluzación más detallada del modelo, pero puede requerir más cómputo.\n"],"metadata":{"id":"kLXToyeZyo01"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","\n","# Removemos los nombres de las características del dataset\n","X = X.values\n","y = y.values\n","# Definimos los hiperparámetros\n","learning_rate = 0.01\n","num_iterations = 1000\n","num_folds = 5\n","\n","# Inicializamos un arreglo para guardar las precisiones\n","accuracy_scores = []\n","\n","# Realizamos cross-validation\n","fold_size = len(X) // num_folds\n","for i in range(num_folds):\n","    # SDibidimos los datos en para entrenamiento y testeo para el fold correspondiente\n","    X_train = np.concatenate((X[:i * fold_size], X[(i + 1) * fold_size:]), axis=0)\n","    y_train = np.concatenate((y[:i * fold_size], y[(i + 1) * fold_size:]), axis=0)\n","    X_test = X[i * fold_size:(i + 1) * fold_size]\n","    y_test = y[i * fold_size:(i + 1) * fold_size]\n","\n","    # Estandarizamos las características\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","\n","    # Definimos el modelo de regresión logística\n","    class LogisticRegression:\n","        def __init__(self, learning_rate=0.01, num_iterations=1000):\n","            self.learning_rate = learning_rate\n","            self.num_iterations = num_iterations\n","\n","        def sigmoid(self, z):\n","            return 1 / (1 + np.exp(-z))\n","\n","        def fit(self, X, y):\n","            num_samples, num_features = X.shape\n","            self.weights = np.zeros(num_features)\n","\n","            for _ in range(self.num_iterations):\n","                linear_model = np.dot(X, self.weights)\n","                y_pred = self.sigmoid(linear_model)\n","                gradient = np.dot(X.T, (y_pred - y)) / num_samples\n","                self.weights -= self.learning_rate * gradient\n","\n","        def predict(self, X):\n","            linear_model = np.dot(X, self.weights)\n","            y_pred = self.sigmoid(linear_model)\n","            y_pred_class = np.where(y_pred >= 0.5, 1, 0)\n","            return y_pred_class\n","\n","    # Creamos una instancia del modelo\n","    model = LogisticRegression(learning_rate=learning_rate, num_iterations=num_iterations)\n","\n","    # Ajustamos el modelo a los datos de entrenamiento\n","    model.fit(X_train_scaled, y_train)\n","\n","    # Realizamos predicciones en los datos de prueba\n","    y_pred = model.predict(X_test_scaled)\n","\n","    # Calculamos la precisión para el fold correspondiente\n","    accuracy = np.mean(y_pred == y_test)\n","    accuracy_scores.append(accuracy)\n","\n","# Calculamos el promedio de la precisión sobre todos los folds\n","average_accuracy = np.mean(accuracy_scores)\n","print(\"Average Accuracy:\", average_accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luG5FuO6y4Gj","executionInfo":{"status":"ok","timestamp":1685502040516,"user_tz":300,"elapsed":481,"user":{"displayName":"Laura Catalina Garcia Arevalo","userId":"14956040326510211844"}},"outputId":"4f56915e-4eda-44aa-f648-6e0d7a1c2b47"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Accuracy: 0.9620437956204378\n"]}]},{"cell_type":"markdown","source":["**(iii) Árboles de decisión**\n","\n","Este es un algoritmo conocido por su efectividad en tareas de clasificación y regresión. Para ajustar los hiperparámetros de este modelo se pueden realizar los mismos métodos que se mencionaron en el caso anterior, adicionalmente existen librerías que ajustan los hiperparámetros automáticamente como por ejemplo ' GridSearchCV' o 'RandomizedSearchCV'."],"metadata":{"id":"CzR9jhxQzeYP"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","\n","# Definimos los hiperparámetros\n","num_folds = 5\n","max_depth = 3  # Máxima profundidad del árbol de decisión\n","min_samples_split = 2  # Mínimum número de samples requeridos para dividir\n","# un nodo interno\n","\n","# Inicializamos un arreglo para guardar las precisiones\n","accuracy_scores = []\n","\n","# Hacemos cross-validation\n","kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Definimos el clasificador del árbol de decisión con los hiperparámetros especificados\n","    model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n","\n","    # Ajustamos el modelo a los datos de entrenamiento\n","    model.fit(X_train, y_train)\n","\n","    # Hacer predicciones sobre los datos de prueba\n","    y_pred = model.predict(X_test)\n","\n","    # Calculamos la precisión sobre el foldn actual\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracy_scores.append(accuracy)\n","\n","# Calculamos el promedio de la precisión sobre todos los folds\n","average_accuracy = np.mean(accuracy_scores)\n","print(\"Average Accuracy:\", average_accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWUV5QF80lOU","executionInfo":{"status":"ok","timestamp":1685502283115,"user_tz":300,"elapsed":232,"user":{"displayName":"Laura Catalina Garcia Arevalo","userId":"14956040326510211844"}},"outputId":"ea2003a3-50d2-453d-886d-b2448ba120d2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Accuracy: 0.9271161247511612\n"]}]},{"cell_type":"markdown","source":["**(iv) KNN**\n","\n","Para ajustar los hiperparámetros en este modelo existen distintos métodos, entre los más comunes están el número de neighbors, el cual entre más grande $k$ hay mayor suavidad de la barrera de decisión; la métrica usada, weighting scheme, feature scaling, reducción de dimensionalidad entre otros."],"metadata":{"id":"zMcOFrde2L2X"}},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","\n","def euclidean_distance(x1, x2):\n","    return np.sqrt(np.sum((x1 - x2) ** 2))\n","\n","class KNNClassifier:\n","    def __init__(self, k=5):\n","        self.k = k\n","\n","    def fit(self, X, y):\n","        self.X_train = X\n","        self.y_train = y\n","\n","    def predict(self, X):\n","        y_pred = [self._predict_single(x) for x in X]\n","        return np.array(y_pred)\n","\n","    def _predict_single(self, x):\n","        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n","        k_indices = np.argsort(distances)[:self.k]\n","        k_nearest_labels = [self.y_train[i] for i in k_indices]\n","        most_common = Counter(k_nearest_labels).most_common(1)\n","        return most_common[0][0]\n","\n","\n","# Inicializamos y ajustamos el clasificador KNN\n","knn = KNNClassifier(k=5)\n","knn.fit(X_train, y_train)\n","\n","# Hacemos predicciones sobre los datos de testeo\n","y_pred = knn.predict(X_test)\n","\n","# Calculamos la precisión del modelo\n","accuracy = np.mean(y_pred == y_test)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asFIQUqX2NKl","executionInfo":{"status":"ok","timestamp":1685500574801,"user_tz":300,"elapsed":2427,"user":{"displayName":"Laura Catalina Garcia Arevalo","userId":"14956040326510211844"}},"outputId":"5dd03b6f-d0c5-4665-9f78-228f7e2ed99a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["De los modelos implementados podemos concluir que el que nos brinda mayor precisión es KNN, además, que existen distintos métodos para ajustar los hiperparámetros y que estos deben ser seleccionados de acuerdo al modelo, el dataset y la capacidad de cómputo."],"metadata":{"id":"jcamJcMEx3Nj"}}]}